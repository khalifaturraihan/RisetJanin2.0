# -*- coding: utf-8 -*-
"""Riset Janin prototype 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ub6-QpxIT-nWNEmHGZ8Vc1CpJJqAVPaH
"""

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

from collections import Counter
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn import metrics
from sklearn.metrics import roc_curve, auc, roc_auc_score

from google.colab import drive
drive.mount('/content/gdrive')

data_df=pd.read_csv('gdrive/My Drive/DeepLearning/FETAL_HEALTH.csv')
data_df.sample(5)

data_df.shape

jumlah_data = len(data_df)

print("Jumlah data dalam dataset:", jumlah_data)

duplicate_rows_data = data_df[data_df.duplicated()]
print("number of duplicate rows: ", duplicate_rows_data.shape)
data_df.drop_duplicates(inplace=True)

# Checking for missing values and categorical variables in the dataset
data_df.info()

# Visualizing the missing values in the dataset,
missing_values = msno.bar(data_df, figsize = (16,5),color = "#483D8B")

print(data_df.columns.values)

# Doing Univariate Analysis for statistical description and understanding of dispersion of data
data_df.describe().T

# Evaluating distributions of the features
hist_plot = data_df.hist(figsize = (20,20), color = "#483D8B")

# Evaluating the target column and checking for imbalance of the data,
colors=["#483D8B","#4682B4", "#87CEFA"]
ax = sns.countplot(data= data_df, x="fetal_health", palette=colors)
ax.bar_label(ax.containers[0])
plt.show()

# Examining correlation matrix using heatmap
cmap = sns.diverging_palette(205, 133, 63, as_cmap=True)
cols = (["#B0E0E6", "#87CEFA", "#4682B4", "#CD853F", "#DEB887", "#FAEBD7"])

corrmat= data_df.corr()

f, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corrmat,cmap=cols,annot=True)
plt.show()

# Defining independent and dependent attributes in training and test sets
X=data_df.drop(["fetal_health"],axis=1)
y=data_df["fetal_health"]

# Setting up a standard scaler for the features and analyzing it thereafter
col_names = list(X.columns)
s_scaler = StandardScaler()
X_scaled= s_scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=col_names)
X_scaled.describe().T

features=['accelerations', 'fetal_movement','uterine_contractions', 'light_decelerations', 'severe_decelerations',
           'prolongued_decelerations', 'abnormal_short_term_variability',
             'percentage_of_time_with_abnormal_long_term_variability']

#Plotting the scaled features using boxen plots
plt.figure(figsize=(20,10))
sns.boxenplot(data = X_scaled,palette = colors)
plt.xticks(rotation=60)
plt.show()

# Splitting the training and test variables
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=25)

"""**Membuat Machine Learning Model Menggunakan Algoritma Random Forest**"""

# Building pipelines of model for various classifiers

pipeline_lr = Pipeline([('lr_classifier',LogisticRegression())])

pipeline_dt = Pipeline([('dt_classifier',DecisionTreeClassifier())])

pipeline_gbcl = Pipeline([('gbcl_classifier',GradientBoostingClassifier())])

pipeline_rf = Pipeline([('rf_classifier',RandomForestClassifier())])

pipeline_knn = Pipeline([('knn_classifier',KNeighborsClassifier())])

# List of all the pipelines
pipelines = [pipeline_lr, pipeline_dt, pipeline_gbcl, pipeline_rf, pipeline_knn]

# Dictionary of pipelines and classifier types for ease of reference
pipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'Gradient Boost', 3:'RandomForest', 4: 'KNN'}

# Fitting the pipelines
for pipe in pipelines:
    pipe.fit(X_train, y_train)

cv_results_accuracy = []
for i, model in enumerate(pipelines):
    cv_score = cross_val_score(model, X_train,y_train, cv=12)
    cv_results_accuracy.append(cv_score)
    print("%s: %f " % (pipe_dict[i], cv_score.mean()))

"""**Random Forest Classifier**"""

# Model dasar RF dengan parameter default:

random_forest = RandomForestClassifier()
random_forest_mod = random_forest.fit(X_train, y_train)
print(f"Baseline Random Forest: {round(random_forest_mod.score(X_test, y_test), 3)}")

pred_random_forest = random_forest_mod.predict(X_test)

scores_RF = cross_val_score(random_forest, X_train, y_train, cv = 10, n_jobs = 2, scoring = "accuracy")

print(f"Scores(Cross validate) for Random forest model:\n{scores_RF}")
print(f"CrossValMeans: {round(scores_RF.mean(), 2)}")
print(f"CrossValStandard Deviation: {round(scores_RF.std(), 2)}")

"""**Grid Search CV**"""

params_RF = {"min_samples_split": [2, 6, 20],
              "min_samples_leaf": [1, 4, 16],
              "n_estimators" :[100,200,300,400],
              "criterion": ["entropy"]
              }

GridSearchCV_RF = GridSearchCV(estimator=RandomForestClassifier(),
                                param_grid=params_RF,
                                cv=2,
                                verbose=1,
                                n_jobs=2,
                                scoring="accuracy",
                                return_train_score=True
                                )

# Fit model with train data
GridSearchCV_RF.fit(X_train, y_train);

best_estimator_RF = GridSearchCV_RF.best_estimator_
print(f"Best estimator for RF model:\n{best_estimator_RF}")

best_params_RF = GridSearchCV_RF.best_params_
print(f"Best parameter values for RF model:\n{best_params_RF}")

best_score_RF = GridSearchCV_RF.best_score_
print(f"Best score for RF model: {round(best_score_RF, 3)}")

"""**Testing Stage**"""

# Testing with the best parameters,
random_forest = RandomForestClassifier(criterion="entropy", n_estimators=300, min_samples_leaf=1, min_samples_split=6, random_state=42)
random_forest_mod = random_forest.fit(X_train, y_train)
pred_random_forest = random_forest_mod.predict(X_test)

score_random_forest_train = random_forest_mod.score(X_train, y_train)
score_random_forest_test = random_forest_mod.score(X_test, y_test)

print(f"R^2(coefficient of determination) on training set = {round(score_random_forest_train, 3)}")
print(f"R^2(coefficient of determination) on testing set = {round(score_random_forest_test, 3)}")

# Getting score on the Test set,

pred_random_forest = pipeline_rf.predict(X_test)
accuracy = accuracy_score(y_test, pred_random_forest)
print(f" Testing Score of the model is {accuracy}")

# Getting the Classification report
print(classification_report(y_test, pred_random_forest))

# Getting the Confusion matrix
plt.subplots(figsize=(12,8))
cf_matrix = confusion_matrix(y_test, pred_random_forest)
sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap='viridis',annot = True, annot_kws = {'size':20})
plt.show()

"""# Mengimport model Ke pickle"""

import pickle

# Melatih model dengan parameter terbaik
random_forest = RandomForestClassifier(criterion="entropy", n_estimators=300, min_samples_leaf=1, min_samples_split=6, random_state=42)
random_forest_mod = random_forest.fit(X_train, y_train)

# Menyimpan model menggunakan pickle
with open('random_forest_model.pkl', 'wb') as file:
    pickle.dump(random_forest_mod, file)

# Memuat model dari file pickle
with open('random_forest_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Menggunakan model yang telah dimuat untuk melakukan prediksi
preds = loaded_model.predict(X_test)

pred_random_forest = random_forest_mod.predict(X_test)

round(accuracy_score(y_test, pred_random_forest),3)